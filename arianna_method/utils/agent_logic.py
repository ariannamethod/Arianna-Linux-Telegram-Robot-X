"""
Universal Agent Logic Module - –æ–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ Arianna Method

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (@timestamp)
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (10 —Å–æ–æ–±—â–µ–Ω–∏–π –≤–æ–∫—Ä—É–≥)
- –§–∞–π–ª–æ–≤—ã—Ö –¥–∏—Å–∫—É—Å—Å–∏–π
- –ü–∞–º—è—Ç–∏ –∏ continuity
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Tommy, Lizzie, Monday –∏ –≤—Å–µ–º–∏ –±—É–¥—É—â–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏.
"""

from __future__ import annotations

import re
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from .vector_store import SQLiteVectorStore, embed_text


class AgentLogic:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, agent_name: str, log_dir: Path, db_path: Path, resonance_db_path: Path):
        self.agent_name = agent_name
        self.log_dir = log_dir
        self.db_path = db_path
        self.resonance_db_path = resonance_db_path
        self.vector_store = SQLiteVectorStore(log_dir / "vectors.db")
        
    def extract_citations(self, message: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–∏—Ç–∞—Ç—ã —Ñ–æ—Ä–º–∞—Ç–∞ @timestamp –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        return re.findall(r"@([0-9T:-]+)", message)
    
    def fetch_context(self, timestamp: str, radius: int = 10) -> List[Tuple[str, str, str]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ timestamp
        
        Args:
            timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            radius: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ
            
        Returns:
            List of (timestamp, type, message) tuples
        """
        with sqlite3.connect(self.db_path, timeout=30) as conn:
            cur = conn.execute("SELECT rowid FROM events WHERE ts = ?", (timestamp,))
            row = cur.fetchone()
            if not row:
                return []
                
            rowid = row[0]
            start = max(rowid - radius, 1)
            end = rowid + radius
            
            cur = conn.execute(
                "SELECT ts, type, message FROM events "
                "WHERE rowid BETWEEN ? AND ? ORDER BY rowid",
                (start, end),
            )
            return cur.fetchall()
    
    async def build_context_block(self, message: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –±–ª–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏"""
        citations = self.extract_citations(message)
        if not citations:
            return ""
            
        blocks: List[str] = []
        for ts in citations:
            ctx = self.fetch_context(ts)
            if ctx:
                formatted = "\n".join(f"[{t}] {m}" for t, _, m in ctx)
                blocks.append(formatted)
                
        if blocks:
            return "Relevant context:\n" + "\n--\n".join(blocks) + "\n\n"
        return ""
    
    def log_event(self, message: str, log_type: str = "info") -> None:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤"""
        # JSON log file
        log_file = self.log_dir / f"{self.agent_name}_{datetime.now().strftime('%Y-%m-%d')}.jsonl"
        entry = {
            "timestamp": datetime.now().isoformat(),
            "type": log_type,
            "message": message,
            "agent": self.agent_name
        }
        
        with open(log_file, "a", encoding="utf-8") as f:
            import json
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        
        # SQLite database
        with sqlite3.connect(self.db_path, timeout=30) as conn:
            conn.execute(
                "INSERT INTO events (ts, type, message) VALUES (?, ?, ?)",
                (datetime.now().isoformat(), log_type, message),
            )
    
    def update_resonance(self, message: str, response: str, 
                        role: str = "agent", sentiment: str = "active") -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ–±—â–∏–π –∫–∞–Ω–∞–ª —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞"""
        resonance_depth = self._calculate_resonance_depth(message, response)
        summary = f"{self.agent_name}: {response[:100]}..."
        
        with sqlite3.connect(self.resonance_db_path, timeout=30) as conn:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            try:
                conn.execute("SELECT resonance_depth FROM resonance LIMIT 1")
            except sqlite3.OperationalError:
                # –ö–æ–ª–æ–Ω–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º
                conn.execute("ALTER TABLE resonance ADD COLUMN resonance_depth REAL DEFAULT 0.0")
            
            conn.execute(
                "INSERT INTO resonance (ts, agent, role, sentiment, resonance_depth, summary) VALUES (?, ?, ?, ?, ?, ?)",
                (
                    datetime.now().isoformat(),
                    self.agent_name,
                    role,
                    sentiment,
                    resonance_depth,
                    summary,
                ),
            )
    
    def _calculate_resonance_depth(self, message: str, response: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≥–ª—É–±–∏–Ω—É —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞"""
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞
        resonance_markers = [
            "resonate", "amplify", "reflect", "mirror", "echo", 
            "deeper", "unfold", "recursive", "paradox", "entropy",
            "chaos", "pattern", "emergence", "connection"
        ]
        
        response_lower = response.lower()
        marker_count = sum(1 for marker in resonance_markers if marker in response_lower)
        
        # Normalize to 0-1 scale
        return min(marker_count / 8.0, 1.0)
    
    def search_context(self, query: str, top_k: int = 5) -> List[str]:
        """–ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        embedding = embed_text(query)
        hits = self.vector_store.query_similar(embedding, top_k)
        return [h.content for h in hits]
    
    async def process_file_context(self, path: str, agent_style_formatter=None) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            agent_style_formatter: –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –≤ —Å—Ç–∏–ª–µ –∞–≥–µ–Ω—Ç–∞
        """
        from .context_neural_processor import parse_and_store_file
        
        try:
            result = await parse_and_store_file(path)
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            lines = result.split('\n')
            tags = ""
            summary = ""
            relevance = 0.0
            
            for line in lines:
                if line.startswith("Tags: "):
                    tags = line[6:]
                elif line.startswith("Summary: "):
                    summary = line[9:]
                elif line.startswith("Relevance: "):
                    try:
                        relevance = float(line[11:])
                    except ValueError:
                        relevance = 0.0
            
            # –ë–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            response_data = {
                "path": path,
                "tags": tags,
                "summary": summary,
                "relevance": relevance,
                "raw_result": result
            }
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if agent_style_formatter:
                response = agent_style_formatter(response_data)
            else:
                # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                response = f"üìÅ File processed: {path}\n"
                if summary:
                    response += f"üìù Summary: {summary}\n"
                    response += f"üè∑Ô∏è Tags: {tags}\n"
                    response += f"‚ö° Relevance: {relevance:.2f}"
                else:
                    response += f"‚ö†Ô∏è Could not extract summary.\n{result}"
            
            # –õ–æ–≥–∏—Ä—É–µ–º
            log_message = f"Processed {path}: {summary[:100] if summary else 'no summary'}"
            self.log_event(log_message)
            
            return response
            
        except Exception as e:
            error_msg = f"üí• Error processing {path}: {str(e)}"
            self.log_event(f"File processing error: {str(e)}", "error")
            return error_msg


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤
_agent_logics: Dict[str, AgentLogic] = {}


def get_agent_logic(agent_name: str, log_dir: Path, db_path: Path, resonance_db_path: Path) -> AgentLogic:
    """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å AgentLogic –¥–ª—è –∞–≥–µ–Ω—Ç–∞"""
    if agent_name not in _agent_logics:
        _agent_logics[agent_name] = AgentLogic(agent_name, log_dir, db_path, resonance_db_path)
    return _agent_logics[agent_name]


# Convenience —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def extract_and_build_context(message: str, agent_logic: AgentLogic) -> str:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    return await agent_logic.build_context_block(message)


def create_agent_file_formatter(agent_name: str, style_markers: Dict[str, str]) -> callable:
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä —Ñ–∞–π–ª–æ–≤ –≤ —Å—Ç–∏–ª–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
    
    Args:
        agent_name: –ò–º—è –∞–≥–µ–Ω—Ç–∞
        style_markers: –°–ª–æ–≤–∞—Ä—å —Å —ç–º–æ–¥–∑–∏ –∏ —Ñ—Ä–∞–∑–∞–º–∏ –∞–≥–µ–Ω—Ç–∞
    """
    def formatter(data: Dict[str, Any]) -> str:
        path = data["path"]
        tags = data["tags"]
        summary = data["summary"] 
        relevance = data["relevance"]
        
        if summary and len(summary) > 20:
            response = f"{style_markers.get('file_icon', 'üìÅ')} File processed: {path}\n\n"
            response += f"{style_markers.get('tags_icon', 'üìã')} Tags: {tags}\n"
            response += f"{style_markers.get('summary_icon', 'üìù')} Summary: {summary}\n"
            response += f"{style_markers.get('relevance_icon', '‚ö°')} Relevance: {relevance:.2f}\n\n"
            
            # –ê–≥–µ–Ω—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            if relevance > 0.5:
                response += style_markers.get('high_relevance', 'üí• High relevance detected!')
            elif relevance > 0.2:
                response += style_markers.get('medium_relevance', '‚ö° Moderate relevance detected.')
            else:
                response += style_markers.get('low_relevance', 'üìä Basic processing complete.')
        else:
            response = f"‚ö†Ô∏è File processed: {path}\n\nCould not extract meaningful summary."
            
        return response
    
    return formatter
